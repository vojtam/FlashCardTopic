{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import ufal.morphodita\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import LiteLLM, MaximalMarginalRelevance\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "questions_csv_path = data_dir / \"questions_cleaned_filtered.csv\"\n",
    "resource_set_path = data_dir / \"umimeprogramovatcz-system_resource_set.csv\"\n",
    "\n",
    "questions_df = pl.read_csv(questions_csv_path, separator=\",\")\n",
    "resource_set_df = pl.read_csv(resource_set_path, separator=\";\")\n",
    "\n",
    "with open(\"rs_filtered.pickle\", \"rb\") as handle:\n",
    "    rc_dict = pickle.load(handle)\n",
    "\n",
    "stopwords = []\n",
    "\n",
    "with open(\"stopwords-cs.txt\", \"r\") as f:\n",
    "    for stopword in f:\n",
    "        stopwords.append(stopword.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_ids = [int(rs_id) for rs_id in rc_dict.keys()]\n",
    "questions_df = questions_df.filter(pl.col(\"rs\").is_in(rs_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) setup pipeline building blocks\n",
    "\n",
    "### 1. embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tagger = ufal.morphodita.Tagger.load(\n",
    "            \"czech-morfflex2.0-pdtc1.0-220710/czech-morfflex2.0-pdtc1.0-220710.tagger\"\n",
    "        )\n",
    "        self.converter = ufal.morphodita.TagsetConverter.newStripLemmaIdConverter(\n",
    "            self.tagger.getMorpho()\n",
    "        )\n",
    "        self.tokenizer = self.tagger.newTokenizer()\n",
    "        self.forms = ufal.morphodita.Forms()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        self.tokenizer.setText(text)\n",
    "        self.tokenizer.nextSentence(self.forms, None)\n",
    "\n",
    "        lemmas = ufal.morphodita.TaggedLemmas()\n",
    "        self.tagger.tag(self.forms, lemmas)\n",
    "\n",
    "        self.converter.convertAnalyzed(lemmas)\n",
    "\n",
    "        raw_lemmas = list([lemma.lemma for lemma in lemmas if lemma.lemma.isalpha()])\n",
    "        return raw_lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Representation models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_llm = False\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "Mám téma (topic), které se vztahuje k následujícím kvízovým otázkám:\n",
    "[DOCUMENTS]\n",
    "Téma lze popsat následujícími klíčovými slovy: [KEYWORDS]\n",
    "Na základě informací výše, extrahuj krátký popisek tématu, použij češtinu a nepřidávej žádné další informace, délka popisku nechť je mezi 3 a 7 slovy. Popisek uveď v následujícím formátu:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "\n",
    "representation_models = [KeyBERTInspired(), MaximalMarginalRelevance(0.5)]\n",
    "if use_llm:\n",
    "    representation_model_LLM = LiteLLM(\n",
    "        model=\"perplexity/sonar-pro\", prompt=PROMPT, nr_docs=4\n",
    "    )\n",
    "    representation_models.append(representation_model_LLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=stopwords + [\"img\", \"pravda\", \"nepravda\"],\n",
    "    tokenizer=LemmaTokenizer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Class TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=False, reduce_frequent_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Assemble and run the pipeline for each of the Resource Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = [rc_id for rc_id in rc_dict.keys()]\n",
    "selected_rs_names = [rc_dict[id] for id in selected_ids]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for id in selected_ids:\n",
    "    filtered_questions_df = questions_df.filter(pl.col(\"rs\") == id)\n",
    "    docs_np = filtered_questions_df[\"question_correct\"].to_numpy().flatten()\n",
    "    docs = docs_np.tolist()\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_models,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        verbose=True,\n",
    "        # zeroshot_topic_list=zeroshot_topic_list,\n",
    "        # zeroshot_min_similarity=0.40,\n",
    "        min_topic_size=5,\n",
    "        umap_model=umap_model,\n",
    "        top_n_words=5,\n",
    "        language=\"multilingual\",\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    if -1 in topics:\n",
    "        new_topics = topic_model.reduce_outliers(\n",
    "            docs, topics, probabilities=probs, strategy=\"probabilities\"\n",
    "        )\n",
    "\n",
    "        topic_model.update_topics(\n",
    "            docs,\n",
    "            topics=new_topics,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            representation_model=representation_models,\n",
    "        )\n",
    "        documents = pd.DataFrame({\"Document\": docs, \"Topic\": new_topics})\n",
    "        topic_model._update_topic_size(documents)\n",
    "\n",
    "    df_docs = pl.from_pandas(topic_model.get_document_info(docs))\n",
    "    df_docs = df_docs.join(df_docs.group_by(\"Topic\").len(), on=\"Topic\")\n",
    "    df_docs = df_docs.join(\n",
    "        filtered_questions_df, left_on=\"Document\", right_on=\"question_correct\"\n",
    "    )\n",
    "    df_docs = df_docs.with_columns(\n",
    "        rs_name=pl.Series(\n",
    "            \"rc_name\", values=[rc_dict[rs_id] for rs_id in df_docs[\"rs\"].to_list()]\n",
    "        )\n",
    "    )\n",
    "    dfs.append(df_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Write the results to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = pl.concat(dfs)\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"docs_topics_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(df_docs.to_dicts(), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs_topics = (\n",
    "    df_docs.select([\"Document\", \"Topic\", \"Name\", \"len\", \"rs_name\", \"successRate\"])\n",
    "    .group_by(\"Topic\", \"Name\", \"len\", \"rs_name\")\n",
    "    .agg(pl.col(\"Document\"), pl.col(\"successRate\"))\n",
    ")\n",
    "df_docs_topics = df_docs_topics.rename({\"len\": \"Count\"})\n",
    "\n",
    "df_docs_topics = df_docs_topics.join(\n",
    "    df_docs_topics.group_by(\"rs_name\").len().rename({\"len\": \"topic_count\"}),\n",
    "    on=\"rs_name\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hierarchical json for treemap visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for _, rs_row in df_docs_topics.group_by(\"rs_name\"):\n",
    "    topic_children = []\n",
    "    for i, row in rs_row.group_by([\"Name\", \"rs_name\"]):\n",
    "        children = []\n",
    "        for docs in row[\"Document\"].to_list():\n",
    "            for doc_i, doc in enumerate(docs):\n",
    "                succ_rate = row[\"successRate\"].to_list()[0][doc_i]\n",
    "                children.append(\n",
    "                    {\n",
    "                        \"name\": doc,\n",
    "                        # \"value\": 1 / row[\"Count\"].item(),\n",
    "                        \"value\": succ_rate\n",
    "                        if succ_rate > 0\n",
    "                        else 30,  # for questions that have not been answered yet, I give default values of 30 so that they are shown in treemap\n",
    "                        \"originalValue\": succ_rate,\n",
    "                        \"label\": {\"fontSize\": 12},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        topic_children.append(\n",
    "            {\n",
    "                \"name\": row[\"Name\"].item(),\n",
    "                \"value\": row[\"Count\"].item(),\n",
    "                \"label\": {\"fontSize\": 18, \"fontWeight\": \"bold\", \"color\": \"#ffffff\"},\n",
    "                \"children\": children,\n",
    "            }\n",
    "        )\n",
    "    data.append(\n",
    "        {\n",
    "            \"name\": rs_row[\"rs_name\"][0],\n",
    "            \"value\": rs_row[\"topic_count\"][0],\n",
    "            \"label\": {\"fontSize\": 18, \"fontWeight\": \"bold\", \"color\": \"#ffffff\"},\n",
    "            \"children\": topic_children,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# json_data_string = json.dumps(data, indent=2)\n",
    "\n",
    "with open(\"topics_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
